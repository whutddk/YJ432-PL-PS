/*
Fast Artificial Neural Network Library (fann)
Copyright (C) 2003-2016 Steffen Nissen (steffen.fann@gmail.com)

This library is free software; you can redistribute it and/or
modify it under the terms of the GNU Lesser General Public
License as published by the Free Software Foundation; either
version 2.1 of the License, or (at your option) any later version.

This library is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
Lesser General Public License for more details.

You should have received a copy of the GNU Lesser General Public
License along with this library; if not, write to the Free Software
Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
*/

#ifndef __fann_cascade_h__
#define __fann_cascade_h__

/* Section: FANN Cascade Training
   Cascade training differs from ordinary training in the sense that it starts with an empty neural network
   and then adds neurons one by one, while it trains the neural network. The main benefit of this approach
   is that you do not have to guess the number of hidden layers and neurons prior to training, but cascade 
   training has also proved better at solving some problems.
   
   The basic idea of cascade training is that a number of candidate neurons are trained separate from the 
   real network, then the most promising of these candidate neurons is inserted into the neural network. 
   Then the output connections are trained and new candidate neurons are prepared. The candidate neurons are 
   created as shortcut connected neurons in a new hidden layer, which means that the final neural network
   will consist of a number of hidden layers with one shortcut connected neuron in each.
*/

/* Group: Cascade Training */



#endif
